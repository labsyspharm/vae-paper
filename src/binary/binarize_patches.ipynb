{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2b6ec0-4fa2-4ed0-a48d-d9c56caed630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "import zarr\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13590917-1af9-42be-8d95-4b44e41cb7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed the output of this code through VAE pipeline on o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3eb9ba-5152-4df6-9e2f-7d47a0c20a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_patches(patches, contrast_limits):\n",
    "    \"\"\"\n",
    "    Create binary masks from original images based on specified contrast limits for each channel.\n",
    "\n",
    "    Parameters:\n",
    "    - original_patches (zarr array): The input array of original images with shape (n_channels, height, width).\n",
    "    - contrast_limits (dict): A dictionary where keys are channel names and values are tuples of channel \n",
    "                              number and minimum contrast limit values for the corresponding channel.\n",
    "\n",
    "    Returns:\n",
    "    - binary_patches (zarr array): An array of binary masks with the same shape as original_images, \n",
    "                                   where each pixel is set to 1 if its value in original_images exceeds \n",
    "                                   the corresponding channel's minimum contrast limit, otherwise set to 0.\n",
    "    \"\"\"\n",
    "    channels = [i[0] for i in contrast_limits.values()]\n",
    "    patches = patches[channels, :, :, :]\n",
    "    binary_patches = zarr.zeros_like(patches)\n",
    "\n",
    "    # loop over channels\n",
    "    limits = [i[1] for i in contrast_limits.values()]\n",
    "    for e, limit in enumerate(limits):\n",
    "        # binarize patches according to threshold value\n",
    "        binary_patches[e] = np.where(patches[e] > limit, 1, 0).astype(np.uint16)\n",
    "    \n",
    "    return binary_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5240af9e-753b-4bcd-95bc-318024b4a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_low_quality_patches(binary_patches, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Remove patches from the binary_patches array if the percentage of zeros \n",
    "    in any channel exceeds the given threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - binary_patches (numpy array): The input array of binary images with shape \n",
    "      (n_channels, n_patches, height, width).\n",
    "    - threshold (float): The threshold for the percentage of zeros in a patch above \n",
    "      which if will be removed.\n",
    "\n",
    "    Returns:\n",
    "    - binary_patches_subset (numpy array): The subset of binary_patches array \n",
    "      after removing patches exceeding the threshold.\n",
    "    - indices_to_remove (numpy array): The indices of patches removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate total number of pixels in each patch\n",
    "    total_pixels = binary_patches.shape[2] * binary_patches.shape[3]\n",
    "\n",
    "    # create zero mask (True for zero, False otherwise)\n",
    "    zero_mask = (binary_patches[:] == 0)\n",
    "    \n",
    "    # sum across height (axis 2) and width (axis 3) of each image patch channel to get the zeros count\n",
    "    num_zeros_per_channel_patch = np.sum(zero_mask, axis=(2, 3))\n",
    "\n",
    "    # calculate percentage of zeros in each patch for all channels\n",
    "    percentage_zeros_per_patch = num_zeros_per_channel_patch / total_pixels\n",
    "\n",
    "    # create Boolean mask for each patch: True if any channel has percentage of zeros exceeding threshold\n",
    "    # mask = np.any(percentage_zeros_per_patch > threshold, axis=0)  # work on all channels\n",
    "    mask = percentage_zeros_per_patch[0] > threshold  # work on first channel only\n",
    "    \n",
    "    # locate indices of patches where any channel exceeds the threshold\n",
    "    indices_to_remove = np.where(mask)[0]\n",
    "\n",
    "    # remove patches from the array across all channels\n",
    "    binary_patches_subset = np.delete(binary_patches, indices_to_remove, axis=1)\n",
    "\n",
    "    return binary_patches_subset, indices_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0203dfe-b9db-4d22-824c-75f341a08ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cluster_labels(subset, main, clustering):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters:\n",
    "    - subset (DataFrame): The split dataframe which will receive the clustering column.\n",
    "    - main (DataFrame): The main dataframe containing all the clustering columns.\n",
    "    - clustering (str): The clustering column to append to the split dataframe.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The updated split dataframe with the clustering column added.\n",
    "    \"\"\"\n",
    "    # merge subset dataframe with main (i.e. full) dataframe based on CellID\n",
    "    subset = subset.merge(main[['CellID', clustering]], on='CellID', how='left')\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9ed877b-8166-48db-8612-6daef5f5aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_clusters(subset, clustering, clusters_to_keep):\n",
    "    \"\"\"\n",
    "    Filter the dataframe to keep only specified clusters and save the dropped indices and CellIDs.\n",
    "\n",
    "    Parameters:\n",
    "    - subset (DataFrame): The dataframe to be filtered.\n",
    "    - clustering (str): The clustering column to filter on.\n",
    "    - clusters_to_keep (list): List of clusters to keep.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The filtered dataframe.\n",
    "    - list: The list of indices that were dropped.\n",
    "    - list: The list of CellIDs of the dropped indices.\n",
    "    \"\"\"\n",
    "    # identify rows to drop\n",
    "    mask_to_drop = ~subset[clustering].isin(clusters_to_keep)\n",
    "    indices_to_drop = subset.index[mask_to_drop].tolist()\n",
    "\n",
    "    # filter data subset\n",
    "    filtered_subset = subset[~mask_to_drop].reset_index(drop=True)\n",
    "\n",
    "    return filtered_subset, indices_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb625d13-6a68-4d74-a225-f2808a5831c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_indices_from_zarr(z, indices_to_drop):\n",
    "    \"\"\"\n",
    "    Drop the specified indices from a zarr array.\n",
    "\n",
    "    Parameters:\n",
    "    - z (zarr array): The input zarr array.\n",
    "    - indices_to_drop (list): List of indices to drop.\n",
    "\n",
    "    Returns:\n",
    "    - z: The updated zarr array with specified indices dropped.\n",
    "    \"\"\"\n",
    "    remaining_indices = np.setdiff1d(np.arange(z.shape[1]), indices_to_drop)\n",
    "    \n",
    "    if isinstance(z, zarr.core.Array):\n",
    "        return z.oindex[:, remaining_indices, :, :]\n",
    "    else:\n",
    "        return z[:, remaining_indices, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d54c20-9e6d-417e-bf1d-3a98d2708b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_zarr_directory(directory_path, zip_file_path):\n",
    "    \"\"\"\n",
    "    Zip the contents of the given Zarr directory into a ZIP file.\n",
    "\n",
    "    Parameters:\n",
    "    - directory_path (str): The path to the Zarr directory.\n",
    "    - zip_file_path (str): The path to the output ZIP file.\n",
    "    \"\"\"\n",
    "    dir_path = pathlib.Path(directory_path)\n",
    "    with zipfile.ZipFile(zip_file_path, \"w\", compression=zipfile.ZIP_STORED) as zf:\n",
    "        for f in dir_path.rglob(\"*\"):\n",
    "            zf.write(f, f.relative_to(dir_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f30a8b5-84ae-4a47-9e48-2dae5d0cce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify VAE clustering of interest and corresponding window size\n",
    "tumor_clusters = [0, 1, 3, 5, 6, 21, 22]\n",
    "clustering = 'VAE9_VIG7' # column header in main.csv\n",
    "window_size = 14  # in pixels\n",
    "\n",
    "# channel contrast limits\n",
    "contrast_limits = {\n",
    "    'Keratin_570': (2, 9500), # channel 0 in z_binary orig: \n",
    "    # 'Ecad_488': (3, 5000), # channel 1 in z_binary\n",
    "    # 'PCNA_488': (19, 8000) # channel 2 in z_binary\n",
    "}\n",
    "\n",
    "# load dataframe containing cluster labels\n",
    "main_csv = os.path.join(parent_dir, 'input/main.csv')\n",
    "main = pd.read_csv(main_csv)\n",
    "\n",
    "# out dir\n",
    "out = os.path.join(parent_dir, 'output/binary/binarized_patches')\n",
    "if not os.path.exists(out):\n",
    "    os.makedirs(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d9cfaf6-719b-49d4-9015-a6e197dab9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on train dataset...\n",
      "234670 indices dropped after cluster filtering train dataset\n",
      "14913 additional indices dropped from train dataset after binarization\n",
      "5661 additional indices dropped from train dataset after removing outliers\n",
      "Checking if csv, zarr, and seg_zarr are same length: True\n",
      "\n",
      "Working on test dataset...\n",
      "29151 indices dropped after cluster filtering test dataset\n",
      "1833 additional indices dropped from test dataset after binarization\n",
      "735 additional indices dropped from test dataset after removing outliers\n",
      "Checking if csv, zarr, and seg_zarr are same length: True\n",
      "\n",
      "Working on validate dataset...\n",
      "29298 indices dropped after cluster filtering validate dataset\n",
      "1931 additional indices dropped from validate dataset after binarization\n",
      "701 additional indices dropped from validate dataset after removing outliers\n",
      "Checking if csv, zarr, and seg_zarr are same length: True\n",
      "\n",
      "Image patch filteration and binarization complete.\n"
     ]
    }
   ],
   "source": [
    "data_subsets = ['train', 'test', 'validate']\n",
    "\n",
    "# loop over each data subset and process\n",
    "for subset in data_subsets:\n",
    "    \n",
    "    print(f'Working on {subset} dataset...')\n",
    "    \n",
    "    # read CSV file\n",
    "    data = pd.read_csv(os.path.join(parent_dir, f'input/VAE9_VIG7/1_cellcutter_input/{subset}.csv'))\n",
    "\n",
    "    # add clustering column to data subset\n",
    "    data_clustered = add_cluster_labels(subset=data, main=main, clustering=clustering)\n",
    "    \n",
    "    # filter subset to keep only tumor clusters, save dropped indices\n",
    "    data_filtered, dropped_indices = filter_clusters(\n",
    "        subset=data_clustered, clustering=clustering, clusters_to_keep=tumor_clusters\n",
    "    )\n",
    "    print(f'{len(dropped_indices)} indices dropped after cluster filtering {subset} dataset')\n",
    "    \n",
    "    # drop indices from the corresponding zarr file\n",
    "    zarr_path = (\n",
    "        os.path.join(parent_dir, \n",
    "                     f'input/VAE9_VIG7/2_cellcutter_output_win14/{subset}_thumbnails_{window_size}.zip')\n",
    "    )\n",
    "    z_store = zarr.ZipStore(zarr_path, mode='r')\n",
    "    z = zarr.open(store=z_store)\n",
    "    z_tumor = drop_indices_from_zarr(z=z, indices_to_drop=dropped_indices)\n",
    "\n",
    "    # binarize zarr patches\n",
    "    z_binary = binarize_patches(patches=z_tumor, contrast_limits=contrast_limits) \n",
    "\n",
    "    # drop low quality binarized patches from zarr and CSV file\n",
    "    z_filtered, low_quality_indices = filter_low_quality_patches(binary_patches=z_binary, threshold=0.8)\n",
    "    data_filtered = data_filtered.drop(low_quality_indices).reset_index(drop=True)\n",
    "    print(f'{len(low_quality_indices)} additional indices dropped from {subset} dataset after binarization')\n",
    "\n",
    "    # remove patch outliers\n",
    "    artifact_cellids = pd.read_csv(os.path.join(os.getcwd(), 'artifact_cellids.csv')) \n",
    "    artifact_indices = data_filtered.index[data_filtered['CellID'].isin(artifact_cellids['CellID'])]\n",
    "    data_filtered = data_filtered.drop(artifact_indices).reset_index(drop=True)\n",
    "    z_filtered = drop_indices_from_zarr(z=z_filtered, indices_to_drop=artifact_indices)\n",
    "    print(f'{len(artifact_indices)} additional indices dropped from {subset} dataset after removing outliers')\n",
    "\n",
    "    # save filtered data subset file\n",
    "    data_filtered.to_csv(os.path.join(out, f'{subset}.csv'), index=False)\n",
    "    \n",
    "    # save filtered zarr array and zip\n",
    "    zarr_filtered_path = os.path.join(out, f'{subset}_thumbnails_{window_size}')\n",
    "    zarr_filtered_store = zarr.DirectoryStore(zarr_filtered_path)\n",
    "    zarr_filtered = zarr.open(\n",
    "        store=zarr_filtered_store, mode='w', shape=z_filtered.shape, dtype=z_filtered.dtype\n",
    "    )\n",
    "    zarr_filtered[:] = z_filtered\n",
    "    zip_zarr_directory(zarr_filtered_path, os.path.join(out, f'{subset}_thumbnails_{window_size}.zip'))\n",
    "    shutil.rmtree(zarr_filtered_path)  # remove unzipped directory after zipping\n",
    "\n",
    "    # drop indices from segmentation zarr\n",
    "    seg_zarr_path = (\n",
    "        os.path.join(parent_dir, \n",
    "                     f'input/VAE9_VIG7/2_cellcutter_output_win14/{subset}_thumbnails_{window_size}_seg.zip')\n",
    "    )\n",
    "    seg_store = zarr.ZipStore(seg_zarr_path, mode='r')\n",
    "    z_seg = zarr.open(store=seg_store)\n",
    "    seg_tumor_only = drop_indices_from_zarr(z=z_seg, indices_to_drop=dropped_indices)\n",
    "    seg_filtered = drop_indices_from_zarr(z=seg_tumor_only, indices_to_drop=low_quality_indices)\n",
    "    seg_filtered = drop_indices_from_zarr(z=seg_filtered, indices_to_drop=artifact_indices)\n",
    "\n",
    "    # save filtered seg zarr and zip\n",
    "    seg_filtered_path = os.path.join(out, f'{subset}_thumbnails_{window_size}_seg')\n",
    "    seg_filtered_store = zarr.DirectoryStore(seg_filtered_path)\n",
    "    zarr_seg_filtered = zarr.open(\n",
    "        store=seg_filtered_store, mode='w', shape=seg_filtered.shape, dtype=seg_filtered.dtype\n",
    "    )\n",
    "    zarr_seg_filtered[:] = seg_filtered\n",
    "    zip_zarr_directory(seg_filtered_path, os.path.join(out, f'{subset}_thumbnails_{window_size}_seg.zip'))\n",
    "    shutil.rmtree(seg_filtered_path)  # remove unzipped directory after zipping\n",
    "\n",
    "    print(\n",
    "        'Checking if csv, zarr, and seg_zarr are same length:', \n",
    "        len(data_filtered) == seg_filtered.shape[1] == z_filtered.shape[1]\n",
    "    )\n",
    "    print()\n",
    "\n",
    "print('Image patch filteration and binarization complete.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
